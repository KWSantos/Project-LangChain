# ğŸ¦œğŸ”— LangChain RAG Agent: Projeto de Estudo com Google Gemini

> Um assistente de conversaÃ§Ã£o baseado em documentos (RAG), implementado com LangChain, Google Gemini e Busca HÃ­brida.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![LangChain](https://img.shields.io/badge/LangChain-v0.3-green)
![Google Gemini](https://img.shields.io/badge/AI-Google%20Gemini-orange)
![Status](https://img.shields.io/badge/Status-Educational-yellow)

## ğŸ“– Sobre o Projeto

Este repositÃ³rio Ã© um **projeto pessoal de aprendizado** focado na exploraÃ§Ã£o do framework **LangChain**. O objetivo principal foi construir uma aplicaÃ§Ã£o de **RAG (Retrieval-Augmented Generation)** robusta, capaz de ler documentos PDF e responder a perguntas sobre eles com base em contexto, mantendo o histÃ³rico da conversa.

Diferente de implementaÃ§Ãµes simples, este projeto explora conceitos avanÃ§ados como **Busca HÃ­brida (Hybrid Search)** e persistÃªncia de memÃ³ria conversacional.

### ğŸ¤” O que Ã© o LangChain?

Para quem estÃ¡ chegando agora: o **LangChain** Ã© um framework poderoso projetado para simplificar o desenvolvimento de aplicaÃ§Ãµes que utilizam Grandes Modelos de Linguagem (LLMs). Ele atua como uma "cola", permitindo conectar LLMs (como GPT ou Gemini) a fontes de dados externas (PDFs, Bancos de Dados, Web) e criar fluxos de trabalho complexos (Chains e Agentes).

---

## âš™ï¸ Arquitetura e Funcionalidades

O sistema foi modularizado em pipelines para facilitar o entendimento e a manutenÃ§Ã£o:

1.  **IngestÃ£o de Dados (ETL):**
    * Carrega PDFs de um diretÃ³rio local (`docs/`).
    * Realiza a quebra do texto em pedaÃ§os menores (Chunks) para otimizar o contexto.
    * Gera Embeddings utilizando o modelo do Google.
2.  **Armazenamento Vetorial:**
    * Utiliza o **ChromaDB** para armazenar os vetores semÃ¢nticos dos documentos.
3.  **Retrieval (RecuperaÃ§Ã£o) HÃ­brido:**
    * Este Ã© o diferencial do projeto. Utilizamos um `EnsembleRetriever` que combina:
        * **Busca SemÃ¢ntica (Vector Store):** Entende o significado e o contexto da pergunta (peso 0.6).
        * **Busca por Palavras-chave (BM25):** Garante precisÃ£o em termos tÃ©cnicos ou nomes especÃ­ficos (peso 0.4).
4.  **GeraÃ§Ã£o de Resposta:**
    * Utiliza o modelo **Google Gemini** (via API) para gerar respostas naturais baseadas nos documentos recuperados e no histÃ³rico da conversa.

---

## ğŸ› ï¸ Tecnologias Utilizadas

* **Python 3.12+**
* **LangChain:** OrquestraÃ§Ã£o do fluxo de IA.
* **Google Generative AI (Gemini):** LLM e Embeddings.
* **ChromaDB:** Banco de dados vetorial local.
* **Rank_BM25:** Algoritmo de busca baseada em frequÃªncia de termos.
* **Python-Dotenv:** Gerenciamento de variÃ¡veis de ambiente.

---

## ğŸš€ Como Rodar o Projeto

### PrÃ©-requisitos

* Python instalado.
* Uma API Key do Google AI Studio (Gemini).

### Passo a Passo

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
    cd seu-repositorio
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Linux/Mac
    # ou
    venv\Scripts\activate     # Windows
    ```

3.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```
    *(Caso nÃ£o tenha o arquivo requirements.txt, instale manualmente)*:
    ```bash
    pip install langchain langchain-community langchain-google-genai langchain-chroma chromadb rank_bm25 pypdf python-dotenv
    ```

4.  **Configure as VariÃ¡veis de Ambiente:**
    Crie um arquivo `.env` na raiz do projeto e adicione sua chave:
    ```env
    GOOGLE_API_KEY=sua_chave_aqui_cola_ela_inteira
    ```

5.  **Adicione seus Documentos:**
    Crie uma pasta chamada `docs` na raiz do projeto e coloque seus arquivos PDF lÃ¡.

6.  **Execute a AplicaÃ§Ã£o:**
    ```bash
    python main.py
    ```

---

## ğŸ“‚ Estrutura de Arquivos

```text
ğŸ“ projeto/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                 # Ponto de entrada (Entry point) da aplicaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ pipeline_etl.py         # Carregamento de PDFs, Chunking e CriaÃ§Ã£o do VectorDB
â”œâ”€â”€ ğŸ“„ retriever.py            # ConfiguraÃ§Ã£o da busca HÃ­brida (BM25 + Chroma)
â”œâ”€â”€ ğŸ“„ conversation_chain.py   # ConfiguraÃ§Ã£o da memÃ³ria e da cadeia de conversaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ .env                    # Arquivo de segredos (NÃƒO COMMITAR)
â””â”€â”€ ğŸ“ docs/                   # Coloque seus PDFs aqui
```
---

## ğŸ§  Aprendizados Chave

Durante o desenvolvimento deste projeto, foram consolidados os seguintes conhecimentos:
* Como estruturar um projeto de IA modular.
* A importÃ¢ncia do **Chunking** correto para nÃ£o estourar a janela de contexto do modelo.
* A diferenÃ§a entre busca vetorial (significado) e busca lexical (palavra exata) e como a uniÃ£o das duas (**Hybrid Search**) melhora a precisÃ£o.
* Gerenciamento de memÃ³ria em conversas contÃ­nuas com LLMs.

---

## ğŸ¤ ContribuiÃ§Ã£o

Sinta-se Ã  vontade para fazer um fork deste projeto, abrir issues ou enviar Pull Requests. Este Ã© um ambiente de aprendizado!

---

Desenvolvido por **KauÃª Cruz** ğŸš€
